{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4: Neural Networks\n",
    "\n",
    "This assignment requires a working IPython Notebook installation, which you should already have. If not, please refer to the instructions in Problem Set 2.\n",
    "\n",
    "The programming part is adapted from [Stanford CS231n](http://cs231n.stanford.edu/).\n",
    "\n",
    "### In part 2 (programming) of this assignment, you DO NOT need to make any modification code in this IPython Notebook. Instead you will implement your own simple neural network in the mlp.py file. Please attach your written solutions for part 1 and part 3 in this IPython Notebook.\n",
    "\n",
    "Total: 100 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [30pts] Problem 1: Backprop in a simple MLP\n",
    "This problem asks you to derive all the steps of the backpropagation algorithm for a simple classification network. Consider a fully-connected neural network, also known as a multi-layer perceptron (MLP), with a single hidden layer and a one-node output layer. The hidden and output nodes use an elementwise sigmoid activation function and the loss layer uses cross-entropy loss:\n",
    "<p>\n",
    "$f(z)=\\frac{1}{1+exp(-z))}$\n",
    "<br>\n",
    "$L(\\hat{y},y)=-yln(\\hat{y}) - (1-y)ln(1-\\hat{y})$\n",
    "</p>\n",
    "<p>\n",
    "The computation graph for an example network is shown below. Note that it has an equal number of nodes in the input and hidden layer (3 each), but, in general, they need not be equal. Also, to make the application of backprop easier, we show the <i>computation graph</i> which shows the dot product and activation functions as their own nodes, rather than the usual graph showing a single node for both.\n",
    "</p>\n",
    "\n",
    "<img src=\"mlpgraph.png\" style=\"height:200px;\"></img>\n",
    "\n",
    "The forward and backward computation are given below. NOTE: We assume no regularization, so you can omit the terms involving $\\Omega$.\n",
    "\n",
    "The forward step is: \n",
    "\n",
    "<img src=\"forward.png\" style=\"width:500px;\"></img>\n",
    "\n",
    "and the backward step is:\n",
    "\n",
    "<img src=\"backward.png\" style=\"width:500px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down each step of the backward pass explicitly for all layers, i.e. for the loss and $k=2,1$, compute all gradients above, expressing them as a function of variables $x, y, h, W, b$. \n",
    "We start by giving an example. Note that we have replaced the superscript notation $u^{(i)}$ with $u^i$, and $\\odot$ stands for element-wise multiplication.\n",
    "\n",
    "$ \\nabla_{\\hat{y}}L(\\hat{y},y) =  \\nabla_{\\hat{y}}[-yln(\\hat{y}) - (1-y)ln(1-\\hat{y})] = \\frac{\\hat{y}-y}{(1-\\hat{y})\\hat{y}} = \\frac{h^2-y}{(1-h^2)h^2}$\n",
    "\n",
    "Next, please derive the following.\n",
    "\n",
    "<i>Hint: you should substitute the updated values for the gradient $g$ in each step and simplify as much as possible.</i>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5pts] Q1.1**: $\\nabla_{a^2}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{a^2}J = \\frac{\\partial J}{\\partial h^2}\\cdot \\frac{\\partial h^2}{\\partial a^2} = \\nabla_{\\hat{y}}L(\\hat{y},y) \\cdot f'(a^2) = \\frac{h^2-y}{(1-h^2)h^2}\\cdot h^2(1 - h^2) = h^2-y$$\n",
    "\n",
    "We note here that $f'(x) = f(x)*(1 - f(x))$ where $f(x)$ is the sigmoid function.We now update g to be equal to the obtaine value, $ g = h^2 - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5pts] Q1.2**: $\\nabla_{b^2}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{b^2}J = \\frac{\\partial J}{\\partial h^2}\\cdot \\frac{\\partial h^2}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial b^2} = g\\cdot 1 = g = h^2 - y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5pts] Q1.3**: $\\nabla_{W^2}J$ <br><i>Hint: this should be a vector, since $W^2$ is a vector. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{W^2}J = \\frac{\\partial J}{\\partial h^2}\\cdot \\frac{\\partial h^2}{\\partial a^2}\\cdot{\\frac{\\partial a^2}{\\partial W^2}} =g \\cdot {h^1}^T = (h^2 - y)\\cdot{h^1}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5pts] Q1.4**: $\\nabla_{h^1}J$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{h^1}J = \\frac{\\partial J}{\\partial h^2}\\cdot\\frac{\\partial h^2}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial h^1} = {W^2}^T \\cdot g = {W^2}^T \\cdot (h^2 - y)$$ \n",
    "\n",
    "We now proceed to update the value of g by using the above value, hence now $ g = {W^2}^T \\cdot (h^2 - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5pts] Q1.5**: $\\nabla_{b^1}J$, $\\nabla_{W^1}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we update g by taking the gradient of J wrt $a^1$\n",
    "\n",
    "$$\\nabla_{a^1}J = \\frac{\\partial J}{\\partial h^2}\\cdot \\frac{\\partial h^2}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial h^1}\\cdot \\frac{\\partial h^1}{\\partial a^1} = g \\cdot f'(a^1) = {W^2}^T \\cdot (h^2 - y) \\cdot h^1 \\cdot (1 - h^1)$$\n",
    "\n",
    "Hence we update the value of g to be $g = {W^2}^T \\cdot (h^2 - y) \\cdot h^1 \\cdot (1 - h^1)$\n",
    "\n",
    "We now proceed to calculate the gradients with respect to $b^1$ and $W^1$.\n",
    "<ul>\n",
    "    <li>$\\nabla_{b^1}J = \\frac{\\partial J}{\\partial h^2}\\cdot \\frac{\\partial h^2}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial h^1}\\cdot \\frac{\\partial h^1}{\\partial a^1}\\cdot\\frac{\\partial a^1}{\\partial b^1} = g\\cdot 1 = g = {W^2}^T \\cdot (h^2 - y) \\cdot h^1 \\cdot (1 - h^1)$</li>\n",
    "    <br>\n",
    "    <li>$\\nabla_{W^1}J = \\frac{\\partial J}{\\partial h^2}\\cdot \\frac{\\partial h^2}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial h^1}\\cdot \\frac{\\partial h^1}{\\partial a^1}\\cdot\\frac{\\partial a^1}{\\partial W^1} = g \\cdot {h^0}^T = {W^2}^T \\cdot (h^2 - y) \\cdot h^1 \\cdot (1 - h^1) \\cdot {h^0}^T$</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5pts] Q1.6** Briefly, explain how the computational speed of backpropagation would be affected if it did not include a forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the forward pass, during our backpropogation step we would not have access to the values we need to compute the derivatives at each step. This would mean that we would have to compute them each time for each backpropagation step. Additionaly, we would not be able to reuse the values since they change at each layer. As a result without the forward pass we would have to perform numerous time-consuming computations if our network was larger than the one we have in this example. As a result it would take much longer to run the neural network. Storing the variables in the forward pass requires more memory but the computation time will be significantly reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [50pts] Problem 2 (Programming): Implementing a simple MLP\n",
    "In this problem we will develop a neural network with fully-connected layers, or Multi-Layer Perceptron (MLP). We will use it in classification tasks.\n",
    "\n",
    "In the current directory, you can find a file `mlp.py`, which contains the definition for class `TwoLayerMLP`. As the name suggests, it implements a 2-layer MLP, or MLP with 1 *hidden* layer. You will implement your code in the same file, and call the member functions in this notebook. Below is some initialization. The `autoreload` command makes sure that `mlp.py` is periodically reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlp import TwoLayerMLP\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize a toy model and some toy data, the task is to classify five 4-d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[ 16.24345364  -6.11756414  -5.28171752 -10.72968622]\n",
      " [  8.65407629 -23.01538697  17.44811764  -7.61206901]\n",
      " [  3.19039096  -2.49370375  14.62107937 -20.60140709]\n",
      " [ -3.22417204  -3.84054355  11.33769442 -10.99891267]\n",
      " [ -1.72428208  -8.77858418   0.42213747   5.82815214]]\n",
      "\n",
      "y =  [0 1 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model(actv, std=1e-1):\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerMLP(input_size, hidden_size, num_classes, std=std, activation=actv)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "X, y = init_toy_data()\n",
    "print('X = ', X)\n",
    "print()\n",
    "print('y = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5pts] Q2.1 Forward pass: Sigmoid\n",
    "Our 2-layer MLP uses a softmax output layer (**note**: this means that you don't need to apply a sigmoid on the output) and the multiclass cross-entropy loss to perform classification. Both are defined in Problem Set 2.\n",
    "\n",
    "**Softmax function**: \n",
    "<br> For class j:\n",
    "$$P(y=j|x) = \\frac{\\exp(z_j)}{\\sum_{k=1}^{K} \\exp(z_k)}$$\n",
    "**Multiclass cross-entropy loss function**: \n",
    "<br> y - binary indicator (0 or 1) if class label c is the correct classification  \n",
    "$$J \\ = \\ \\frac{1}{m} \\ \\sum_{i=1}^{m} \\sum_{c=1}^{C} \\ [ \\ -y_{(c)} log(P(y_{(c)}|x^{(i)})) \\ ]$$\n",
    "\n",
    "Please take a look at method `TwoLayerMLP.loss` in the file `mlp.py`. This function takes in the data and weight parameters, and computes the class scores (aka logits), the loss $L$, and the gradients on the parameters. \n",
    "\n",
    "- Complete the implementation of forward pass (up to the computation of `scores`) for the sigmoid activation: $\\sigma(x)=\\frac{1}{1+exp(-x)}$.\n",
    "\n",
    "**Note 1**: Softmax cross entropy loss involves the [log-sum-exp operation](https://en.wikipedia.org/wiki/LogSumExp). This can result in numerical underflow/overflow. Read about the solution in the link, and try to understand the calculation of `loss` in the code.\n",
    "\n",
    "**Note 2**: You're strongly encouraged to implement in a vectorized way and avoid using slower `for` loops. Note that most numpy functions support vector inputs.\n",
    "\n",
    "Check the correctness of your forward pass below. The difference should be very small (<1e-6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1822479803941373\n",
      "Difference between your loss and correct loss:\n",
      "1.9605862711102873e-08\n"
     ]
    }
   ],
   "source": [
    "net = init_toy_model('sigmoid')\n",
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "correct_loss = 1.182248\n",
    "print(loss)\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10pts] Q2.2 Backward pass: Sigmoid\n",
    "- For sigmoid activation, complete the computation of `grads`, which stores the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`.\n",
    "\n",
    "Now debug your backward pass using a numeric gradient check. Again, the differences should be very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 8.048892e-10\n",
      "b2 max relative error: 5.553999e-11\n",
      "W1 max relative error: 1.126755e-08\n",
      "b1 max relative error: 2.035406e-06\n"
     ]
    }
   ],
   "source": [
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "from utils import eval_numerical_gradient\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "\n",
    "# these should all be very small\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e'%(param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5pts] Q2.3 Train the Sigmoid network\n",
    "To train the network we will use stochastic gradient descent (SGD), implemented in `TwoLayerNet.train`. Then we train a two-layer network on toy data.\n",
    "\n",
    "- Implement the prediction function `TwoLayerNet.predict`, which is called during training to keep track of training and validation accuracy.\n",
    "\n",
    "You should get the final training loss around 0.1, which is good, but not too great for such a toy problem.  One problem is that the gradient magnitude for W1 (the first layer weights) stays small all the time, and the neural net doesn't get much \"learning signals\". This has to do with the saturation problem of the sigmoid activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.10926794610680679\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model('sigmoid', std=1e-1)\n",
    "stats = net.train(X, y, X, y,\n",
    "                  learning_rate=0.5, reg=1e-5,\n",
    "                  num_epochs=100, verbose=False)\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history and gradient magnitudes\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['grad_magnitude_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('||W1||')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Gradient magnitude history (W1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5pts] Q2.4 Using ReLU activation\n",
    "The Rectified Linear Unit (ReLU) activation is also widely used: $ReLU(x)=max(0,x)$.\n",
    "\n",
    "- Complete the implementation for the ReLU activation (forward and backward) in `mlp.py`.\n",
    "- Train the network with ReLU, and report your final training loss.\n",
    "\n",
    "Make sure you first pass the numerical gradient check on toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  1.3037878913298206\n",
      "checking gradients\n",
      "W2 max relative error: 3.440708e-09\n",
      "b2 max relative error: 3.865091e-11\n",
      "W1 max relative error: 3.561318e-09\n",
      "b1 max relative error: 8.994864e-10\n"
     ]
    }
   ],
   "source": [
    "net = init_toy_model('relu', std=1e-1)\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "print('loss = ', loss)  # correct_loss = 1.320973\n",
    "\n",
    "# The differences should all be very small\n",
    "print('checking gradients')\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e'%(param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's working, let's train the network. Does the net get stronger learning signals (i.e. gradients) this time? Report your final training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.0178562204869839\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model('relu', std=1e-1)\n",
    "stats = net.train(X, y, X, y,\n",
    "                  learning_rate=0.1, reg=1e-5,\n",
    "                  num_epochs=50, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['grad_magnitude_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('||W1||')\n",
    "plt.title('Gradient magnitude history (W1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data\n",
    "Now that you have implemented a two-layer network that works on toy data, let's try some real data. The MNIST dataset is a standard machine learning benchmark. It consists of 70,000 grayscale handwritten digit images, which we split into 50,000 training, 10,000 validation and 10,000 testing. The images are of size 28x28, which are flattened into 784-d vectors.\n",
    "\n",
    "**Note 1**: the function `get_MNIST_data` requires the `scikit-learn` package. If you previously did anaconda installation to set up your Python environment, you should already have it. Otherwise, you can install it following the instructions here: http://scikit-learn.org/stable/install.html\n",
    "\n",
    "**Note 2**: If you encounter a `HTTP 500` error, that is likely temporary, just try again.\n",
    "\n",
    "**Note 3**: Ensure that the downloaded MNIST file is 55.4MB (smaller file-sizes could indicate an incomplete download - which is possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielvaroli/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:85: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/danielvaroli/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:85: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST\n",
    "from utils import get_MNIST_data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_MNIST_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.5 Train a network on MNIST\n",
    "We will now train a network on MNIST with 64 hidden units in the hidden layer. We train it using SGD, and decrease the learning rate with an exponential rate over time; this is achieved by multiplying the learning rate with a constant factor `learning_rate_decay` (which is less than 1) after each epoch. In effect, we are using a high learning rate initially, which is good for exploring the solution space, and using lower learning rates later to encourage convergence to a local minimum (or [saddle point](http://www.offconvex.org/2016/03/22/saddlepoints/), which may happen more often).\n",
    "\n",
    "- Train your MNIST network with 2 different activation functions: sigmoid and ReLU. \n",
    "\n",
    "We first define some variables and utility functions. The `plot_stats` function plots the histories of gradient magnitude, training loss, and accuracies on the training and validation sets. The `visualize_weights` function visualizes the weights learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized. Both functions help you to diagnose the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size = 64\n",
    "num_classes = 10\n",
    "\n",
    "# Plot the loss function and train / validation accuracies\n",
    "def plot_stats(stats):\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(stats['grad_magnitude_history'])\n",
    "    plt.title('Gradient magnitude history (W1)')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('||W1||')\n",
    "    plt.ylim(0, np.minimum(100,np.max(stats['grad_magnitude_history'])))\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(stats['train_acc_history'], label='train') \n",
    "    plt.plot(stats['val_acc_history'], label='val')\n",
    "    plt.title('Classification accuracy history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Clasification accuracy')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the weights of the network\n",
    "from utils import visualize_grid\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.T.reshape(-1, 28, 28)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10pts] Q2.5.1 Sigmoid network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_net = TwoLayerMLP(input_size, hidden_size, num_classes, activation='sigmoid', std=1e-1)\n",
    "\n",
    "# Train the network\n",
    "sigmoid_stats = sigmoid_net.train(X_train, y_train, X_val, y_val, \n",
    "                                  num_epochs=20, batch_size=100, \n",
    "                                  learning_rate=1e-3,  learning_rate_decay=0.95, \n",
    "                                  reg=0.5, verbose=True)\n",
    "\n",
    "# Predict on the training set\n",
    "train_acc = (sigmoid_net.predict(X_train) == y_train).mean()\n",
    "print('Sigmoid final training accuracy: ', train_acc)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (sigmoid_net.predict(X_val) == y_val).mean()\n",
    "print('Sigmoid final validation accuracy: ', val_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "test_acc = (sigmoid_net.predict(X_test) == y_test).mean()\n",
    "print('Sigmoid test accuracy: ', test_acc)\n",
    "\n",
    "# show stats and visualizations\n",
    "plot_stats(sigmoid_stats)\n",
    "show_net_weights(sigmoid_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10pts] Q2.5.2 ReLU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_net = TwoLayerMLP(input_size, hidden_size, num_classes, activation='relu', std=1e-1)\n",
    "\n",
    "# Train the network\n",
    "relu_stats = relu_net.train(X_train, y_train, X_val, y_val, \n",
    "                            num_epochs=20, batch_size=100,\n",
    "                            learning_rate=1e-3, learning_rate_decay=0.95, \n",
    "                            reg=0.5, verbose=True)\n",
    "# Predict on the training set\n",
    "train_acc = (relu_net.predict(X_train) == y_train).mean()\n",
    "print('ReLU final training accuracy: ', train_acc)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (relu_net.predict(X_val) == y_val).mean()\n",
    "print('ReLU final validation accuracy: ', val_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "test_acc = (relu_net.predict(X_test) == y_test).mean()\n",
    "print('ReLU test accuracy: ', test_acc)\n",
    "\n",
    "# show stats and visualizations\n",
    "plot_stats(relu_stats)\n",
    "show_net_weights(relu_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5pts] Q2.5.3 \n",
    "\n",
    "Based on the outputs of the function call above, we can see that the test accuracy of the ReLU method is around 97%, however the test accuracy of the sigmoid activation function is around 87%, which is higher, hence I would pick ReLU. In addition the ReLU method is sparse, which means that it allows a network to easily obtain sparse representations.Furthermore, with the ReLU the chances of the occurance of vanishing grqdients is reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20pts] Problem 3: Simple Regularization Methods\n",
    "You may have noticed the `reg` parameter in `TwoLayerMLP.loss`, controlling \"regularization strength\". In learning neural networks, aside from minimizing a loss function $\\mathcal{L}(\\theta)$ with respect to the network parameters $\\theta$, we usually explicitly or implicitly add some regularization term to reduce overfitting. A simple and popular regularization strategy is to penalize some *norm* of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10pts] Q3.1:  L2 regularization\n",
    "\n",
    "We can penalize the L2 norm of $\\theta$: we modify our objective function to be $\\mathcal{L}(\\theta) + \\lambda \\|\\theta\\|^2$ where $\\lambda$ is the weight of regularization. \n",
    "We will minimize this objective using gradient descent with step size $\\eta$.\n",
    "Derive the update rule: at time $t+1$, express the new parameters $\\theta_{t+1}$ in terms of the old parameters $\\theta_t$, the gradient $g_t=\\frac{\\partial \\mathcal{L}}{\\partial \\theta_t}$, $\\eta$, and $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"q3-1.png\"></img>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10pts] Q3.2:  L1 regularization\n",
    "\n",
    "Now let's consider L1 regularization: our objective in this case is $\\mathcal{L}(\\theta) + \\lambda \\|\\theta\\|_1$. Derive the update rule. \n",
    "\n",
    "(Technically this becomes *Sub-Gradient* Descent since the L1 norm is not differentiable at 0. But practically it is usually not an issue.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"q3-2.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
