{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Ekjsmds4_n2G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem Set 7 (Total points: 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxmzV7JUbHjx"
   },
   "source": [
    "### Q1. Markov Decision Process [20 points]\n",
    "Imagine an MDP corresponding to playing slot machines in a casino. Suppose you start with $\\$20$ cash to spend in the casino,  and you decide to play until you lose all your money or until you double your money (i.e., increase your cash to at least $\\$40$). There are two slot machines you can choose to play: 1) slot machine X costs $\\$10$ to play and will pay out $\\$20$ with probability 0.05 and will pay $\\$0$ otherwise;\n",
    "and 2) slot machine Y costs $\\$20$ to play and will pay out $\\$30$ with probability 0.01 and $\\$0$ otherwise. As you are playing, you keep choosing machine X or Y at each turn.\n",
    "\n",
    "Write down the MDP that corresponds to the above problem. Clearly specify the state space, action space, rewards and transition probabilities. Indicate which state(s) are terminal. Assume that the discount factor γ = 1.\n",
    "\n",
    "**Notes:** There are several valid ways to specify the MDP, so you have some flexibility in your solution. For example, rewards can take many different forms, but overall you should get a higher reward for stopping when you double your money than when you lose all your money!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_H3z6wR_n2I"
   },
   "source": [
    "## Reinforcement Learning\n",
    "In the remainder of the problem set you will implement the Q-Learning Algorithm to solve the \"Frozen Lake\" problem.\n",
    "We​ ​will​ ​use​ ​OpenAI’s​ ​gym​ ​package​ ​to​ ​develop​ ​our​ ​solution​ ​in​ ​Python.​ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8sgYT6oUKGz"
   },
   "source": [
    "### OpenAI Gym Setup \n",
    "​Read the​ ​set-up​ ​instructions for​ ​Gym​  [here](https://gym.openai.com/docs/).​ ​The​ ​instructions​ ​also​ ​give​ ​a​ ​good​ ​overview​ ​of​ ​the​ ​API​ ​for​ ​this​ ​package,​ ​so​ ​please​ ​read through​ ​it​ ​before​ ​proceeding.​\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQ-qqj1n_n2K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Frozen Lake\n",
    "Instead​ ​of​ ​using​ ​CartPole,​ ​we’re​ ​going​ ​to​ ​be​ ​using​ ​the​ [​​FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment. Read through the code for this environment by following the Github link.​ <br>\n",
    "\n",
    "Winter​ ​is​ ​quickly​ ​approaching,​ ​and​ ​we​ ​have​ ​to​ ​worry​ ​about​ ​navigating​ ​frozen​ ​lakes.​ ​It’s​ ​only early November,​ ​\n",
    "so​ ​the​ ​lakes​ ​haven’t​ ​completely​ ​frozen​ ​and​ ​if​ ​you​ ​make​ ​the​ ​wrong​ ​step​ ​you​ ​may​ ​fall​ ​through. \n",
    "We’ll​ ​need​ ​to​ ​learn​ ​how​ ​to​ ​get​ ​to​ ​our​ ​destination​ ​when​ ​stuck​ ​on​ ​the​ ​ice,​ ​without​ ​falling​ ​in.\n",
    "The​ ​lake​ ​we’re​ ​going​ ​to​ ​consider​ ​is a​ ​square​ ​lake​ ​with​ ​spots​ ​to​ ​step​ ​on​ ​in​ ​the​ ​shape​ ​of​ ​a​ ​grid.​ ​​<br>\n",
    "\n",
    "The surface is described using a 4x4 grid like the following\n",
    "\n",
    "        S F F F \n",
    "        F H F H\n",
    "        F F F H\n",
    "        H F F G\n",
    "\n",
    "​Each​ ​spot​ ​can have​ ​one​ ​of​ ​four​ ​states:\n",
    "- S:​ ​starting​ ​point.\n",
    "- G:​ ​goal​ ​point.\n",
    "- F:​ ​frozen​ ​spot,​ ​where​ ​it’s​ ​safe​ ​to​ ​walk.\n",
    "- H:​ ​hole​ ​in​ ​the​ ​ice,​ ​where​ ​it’s​ ​not​ ​safe​ ​to​ ​walk.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fPxEMPQ_n2L"
   },
   "source": [
    "For example, consider the lake, <br>\n",
    "![alt text](https://cs-people.bu.edu/sunxm/frozen_lake_example.png)\n",
    "\n",
    "There are four possible actions: UP, DOWN, LEFT, RIGHT. Although we​ ​can​ ​see​ ​the​ ​path​ ​we​ ​need​ ​to​ ​walk,​ the agent does not. ​We’re​ ​going​ ​to​ ​train​ ​an​ ​agent​ ​to​ ​discover​ ​this​ ​via​ ​problem solving.​ ​However,​ ​walking​ ​on​ ​ice​ ​isn’t​ ​so​ ​easy!​ ​Sometimes​ ​you​ ​slip​ ​and​ ​aren’t​ ​able​ ​to​ ​take​ ​the​ ​step​ ​you intended.\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZlKYHn-XE2X"
   },
   "source": [
    "#### Q2. Walking on the Frozen Lake [10 points]\n",
    "\n",
    "Write a script that sets up the Frozen Lake environment and takes 10 walks through it, consisting of a maximum 10 randomly sampled actions during each walk. After each step, render the current state, and print out the reward and whether the walk is \"done\", i.e. in the terminal state, because the agent fell into a hole (stop if it is). In your own words, explain how this environment behaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#!pip install gym\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0',is_slippery = True)\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(\"Reward:\",reward)\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Done!\")\n",
    "        break\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHPqSDaE_n2M"
   },
   "source": [
    "#### Q3. Q-Learning [50 points]\n",
    "\n",
    "You will ​implement​ ​Q-learning to solve the problem.​ Assume that the environment has states S and actions A. Use the ​function​ ​signature​ ​provided​ below:\n",
    "\n",
    "``` python\n",
    "def​ q_learning(env,​ ​alpha=0.5,​ gamma=0.95,​ ​epsilon=0.1, num_episodes=500):\n",
    "\"\"\" ​Performs​ ​Q-learning​ ​for​ ​the​ ​given​ ​environment.\n",
    "Initialize​ ​Q​ ​to​ ​all​ ​zeros.\n",
    ":param​ ​env:​ ​Unwrapped​ ​OpenAI​ ​gym​ ​environment.\n",
    ":param​ ​alpha:​ ​Learning​ ​rate​ ​parameter.\n",
    ":param​ ​gamma:​ ​Decay​ ​rate​ (future reward discount) ​parameter.\n",
    ":param​ ​num_episodes:​ ​Number​ ​of​ ​episodes​ ​to​ ​use​ ​for​ ​learning. \n",
    ":return:​ ​Q​ ​table, i.e. a table with the Q value for every <S, A> pair.\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgFa7qID_n2O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The pseudocode for Q-Learning was described in lecture, but for your reference, we provide it here:\n",
    "![alt text](https://cs-people.bu.edu/sunxm/q-learning.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env,alpha=0.5,gamma=0.95,epsilon=0.1,num_episodes = 500, Q_param = 'zeros'):\n",
    "    \n",
    "    if Q_param == 'zeros':   \n",
    "        Q = np.zeros((env.nS,env.nA)) # initialize Q with number of States and number of Actions\n",
    "    else:\n",
    "        Q = np.random.rand(env.nS,env.nA)\n",
    "        \n",
    "    for episode in range(1,num_episodes + 1):\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        \n",
    "        while done == False:\n",
    "            # implement epsilon-greedy\n",
    "            p = np.random.uniform(0,1)\n",
    "            \n",
    "            if p > epsilon:\n",
    "                # exploitation\n",
    "                action = np.argmax(Q[state,:])\n",
    "            else:\n",
    "                # exploration\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            # update the q-table\n",
    "            \n",
    "            Q[current_state,action] = Q[current_state,action] + alpha*(reward + gamma*np.max(Q[new_state,:]) - Q[current_state,action])\n",
    "            \n",
    "            current_state = new_state # update the state\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JHa8uv6_n2N"
   },
   "source": [
    "#### Q4. Main Function [20 points]\n",
    "You also need to implement the main function to solve the entire FrozenLake-v0 problem, including setting up the Gym environment, \n",
    "calling the q-learning function as defined above, printing out the returned Q-table, etc. <br>\n",
    "\n",
    "You should use the $\\epsilon$-greedy algorithm (solving Exploration-Exploitation Dilemma) to generate actions for each state. Try `num_episodes` with different values, e.g. `num_episodes=500, 1000, 5000, 10000`. You should also try different initializations for the Q-table, i.e. Random Initialization and Zero Initialization. \n",
    "\n",
    "Provide​ ​the​ ​final​ ​Q-table​ ​of​ ​​FrozenLake-v0​ for **each <num_episode, init_method>** you have tried [10 points],​ ​and analyze ​what​ you observe [10 points]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization: rand. Num. Episodes 10000. Success rate: 0.380\n",
      "Corresponding Q-table: \n",
      " [[0.21383242 0.19848469 0.19855413 0.16161581]\n",
      " [0.14230616 0.14442371 0.11500114 0.13862891]\n",
      " [0.11896048 0.0074091  0.10162756 0.04164685]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.24157659 0.17303822 0.05045461 0.03885366]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21472466 0.         0.06129642 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.12059774 0.23891304 0.04650528 0.27432943]\n",
      " [0.29956559 0.32984956 0.25450565 0.25211466]\n",
      " [0.2953148  0.2375     0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.13078876 0.43046875 0.2620315  0.36428175]\n",
      " [0.38217554 0.5        0.5        0.31745353]\n",
      " [0.         0.         0.         0.        ]]\n",
      "________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inits = ['zeros','rand']\n",
    "episodes = [500,1000,5000,10000,50000]\n",
    "env = gym.make('FrozenLake-v0',is_slippery = True)\n",
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "env.action_space.np_random.seed(43)\n",
    "        \n",
    "for init in inits:\n",
    "    for ep in episodes:\n",
    "        Q = q_learning(env,num_episodes = ep)\n",
    "\n",
    "        test_runs = 100\n",
    "\n",
    "        total_reward = 0\n",
    "        for run_num in range(1,test_runs + 1):\n",
    "            current_state = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            while done == False:\n",
    "                action = np.argmax(Q[current_state,:])\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                current_state = new_state\n",
    "\n",
    "        success = total_reward/test_runs\n",
    "\n",
    "        print(\"Initialization: %s. Num. Episodes %s. Success rate: %.3f\" % (init,ep,success))\n",
    "        print(\"Corresponding Q-table: \\n\", Q)\n",
    "        print(\"________________________________________________________________\\n\")\n",
    "        \n",
    "        env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtF6U2vg_n2O"
   },
   "source": [
    "#### Additional Instructions\n",
    "If​ ​you’re​ ​new​ ​to​ OpenAI’s​ ​Gym,​ first ​go​ ​through​ ​OpenAI’s​ ​full​ ​tutorial listed​ ​earlier​ ​and​ ​visit​ ​the​ ​Appendix​ ​to​ ​this​ ​homework​ ​before​ ​proceeding.\n",
    "Some additional rules:\n",
    "- Only submit **original code** written entirely by you.\n",
    "- **Permitted​​ non-standard ​​libraries**:​​ ​gym​,​​ ​numpy.\n",
    "- **Only​ ​use​ ​numpy​ ​for​ ​random​ ​sampling,​ ​and​ ​seed​ ​at​ ​the​ ​beginning​ ​of​ ​each​ ​function​ ​with**:\n",
    "np.random.seed(42)\n",
    "- **Unwrap​ ​the​ ​OpenAI​ ​gym​ ​before​ ​providing​ ​them​ ​to​ ​these​ ​functions.** <br>\n",
    "env​ ​=​ ​gym.make(“FrozenLake-v0”).unwrapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjpJpTbV_n2Q"
   },
   "source": [
    "## Appendix\n",
    "\n",
    "This​ ​appendix​ ​includes​ ​references​ ​to​ ​APIs​ ​you​ ​may​ ​find​ ​useful.​ ​If​ ​the​ ​description​ ​sounds​ ​useful,​ ​check out\n",
    "the​ ​respective​ ​package’s​ ​documentation​ ​for​ ​a​ ​much​ ​better​ ​description​ ​than​ ​we​ ​could​ ​provide.\n",
    "\n",
    "#### Numpy\n",
    "- np.zeros:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​all​ ​0s.\n",
    "- np.ones:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​all​ ​1s.\n",
    "- np.eye:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​a​ ​diagonal​ ​matrix​ ​of​ ​1s.\n",
    "- np.random.choice:​ ​Randomly​ ​sample​ ​from​ ​a​ ​list,​ ​allowing​ ​you​ ​to​ ​specify​ ​weights.\n",
    "- np.argmax:​ ​Index​ ​of​ ​the​ ​maximum​ ​element.\n",
    "- np.abs:​ ​Absolute​ ​value.\n",
    "- np.mean:​ ​Average​ ​across​ ​dimensions.\n",
    "- np.sum:​ ​Sum​ ​across​ ​dimensions.\n",
    "\n",
    "### OpenAI Gym\n",
    "- Environment​ ​(unwrapped):<br>\n",
    "env.nS #​ ​Number​ ​of​ ​spaces. <br>\n",
    "env.nA #​ ​Number​ ​of​ ​actions. <br>\n",
    "env.P #​ ​Dynamics​ ​model.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pset7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
